{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPDKDmv6cTevn2C9T4CEsPo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samhita-alla/geolocator/blob/main/geolocator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video Geolocator\n",
        "\n",
        "Uses a [pre-trained GeoEstimation model](https://github.com/TIBHannover/GeoEstimation) to perform video inferencing. \n",
        "\n",
        "* Get the video path. If a YouTube video, download it.\n",
        "* Retrieve video frames.\n",
        "* Perform model inferencing on every video frame.\n",
        "* Apply DBSCAN clustering on the predicted lats and longs.\n",
        "* Get the most dense cluster.\n",
        "* Compute mean of lats and longs of the data points belonging to the dense cluster.\n",
        "* Predict location.\n",
        "\n",
        "## Library dependencies\n",
        "\n",
        "* Katna\n",
        "* Youtube DL\n",
        "* Scikit Learn\n",
        "* PyTorch Lightning\n",
        "* s2sphere\n",
        "* Geopy\n"
      ],
      "metadata": {
        "id": "WqX1d86PkcFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "!pip install -q katna youtube_dl pytorch-lightning s2sphere scikit-learn"
      ],
      "metadata": {
        "id": "3ERC4zjStDOv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import unicode_literals\n",
        "import youtube_dl\n",
        "from pathlib import Path\n",
        "from Katna.config import Video as VideoConfig\n",
        "from Katna.writer import KeyFrameDiskWriter\n",
        "import os\n",
        "from typing import Dict, Any, Tuple\n",
        "\n",
        "MAX_FILESIZE = 10000000\n",
        "\n",
        "def sort_key(key):\n",
        "  file_size = key[\"filesize\"]\n",
        "  if file_size:\n",
        "    return int(key[\"filesize\"])\n",
        "  return 0\n",
        "\n",
        "def validate_extension(selected_format: Dict[str, Any]) -> str:\n",
        "  extension = selected_format.get(\"ext\")\n",
        "  if extension and extension not in map(lambda x: x.replace(\".\", \"\"), VideoConfig.video_extensions):\n",
        "    raise f\"{extension} isn't supported.\"\n",
        "  return extension\n",
        "\n",
        "def extract_youtube_video(url: str) -> Tuple[str, Dict[str, Any]]:\n",
        "  ydl = youtube_dl.YoutubeDL({})\n",
        "\n",
        "  # extra information about the video\n",
        "  info_dict = ydl.extract_info(url, download=False)\n",
        "  formats = info_dict.get(\"formats\", [])\n",
        "\n",
        "  # sort the formats in descending order w.r.t the file size\n",
        "  sorted_formats = sorted(formats, key=sort_key, reverse=True)\n",
        "\n",
        "  # remove \"webm\" formatted videos\n",
        "  filtered_sorted_formats = list(filter(lambda x: x[\"ext\"] != \"webm\", sorted_formats))\n",
        "\n",
        "  # select the best format -- the nearest big number to MAX_FILESIZE\n",
        "  selected_format = {}\n",
        "  for format in filtered_sorted_formats:\n",
        "    file_size = format[\"filesize\"]\n",
        "    if file_size and file_size < MAX_FILESIZE and format[\"vcodec\"] != \"none\":\n",
        "      selected_format = format\n",
        "      break\n",
        "  \n",
        "  # verify if the extension is valid\n",
        "  extension = validate_extension(selected_format)\n",
        "\n",
        "  # extract YT video\n",
        "  videos_path = \"videos\"\n",
        "  ydl_opts = {\"max_filesize\": MAX_FILESIZE, \"format\": selected_format.get(\"format_id\"), \"outtmpl\": f\"{videos_path}/%(id)s.%(ext)s\"}\n",
        "\n",
        "  with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.cache.remove()\n",
        "    ydl.download([url])\n",
        "    saved_location = f\"videos/{info_dict['id']}.{extension}\"\n",
        "\n",
        "  return saved_location, info_dict"
      ],
      "metadata": {
        "id": "UIWrBFjaHGqt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Katna.video import Video\n",
        "\n",
        "NUMBER_OF_FRAMES = 20\n",
        "\n",
        "def capture_frames(video_file_path: str, info_dict: Dict[str, Any]) -> str:\n",
        "  # create a directory to store video frames\n",
        "  frames_directory = f\"selected-frames/{info_dict['id']}\"\n",
        "  shutil.rmtree(frames_directory, ignore_errors=True)\n",
        "  os.makedirs(frames_directory, exist_ok=True)\n",
        "  diskwriter = KeyFrameDiskWriter(location=frames_directory)\n",
        "\n",
        "  vd = Video()\n",
        "  try:\n",
        "    vd.extract_video_keyframes(\n",
        "        no_of_frames=NUMBER_OF_FRAMES,\n",
        "        file_path=video_file_path,\n",
        "        writer=diskwriter\n",
        "    )\n",
        "  except Exception as e:\n",
        "    raise f\"Error capturing the frames: {e}\"\n",
        "\n",
        "  return frames_directory"
      ],
      "metadata": {
        "id": "aPOZgyLDtMAA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "image_dir = None\n",
        "image_parent_dir = \"geolocator-images\"\n",
        "\n",
        "def display_video_frames(frames_directory: str):\n",
        "  frames = glob.glob(f\"{frames_directory}/*.jpeg\")\n",
        "\n",
        "  for frame in frames:\n",
        "    display(Image(filename=frame, width = 200, height = 100))"
      ],
      "metadata": {
        "id": "w470v4dJ0DJu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/yiyixuxu/GeoEstimation.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhchGk0zBR4H",
        "outputId": "8a885b86-ce0c-4fca-f72b-ad706882be09"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GeoEstimation'...\n",
            "remote: Enumerating objects: 451, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 451 (delta 63), reused 98 (delta 46), pack-reused 328\u001b[K\n",
            "Receiving objects: 100% (451/451), 1.94 MiB | 5.27 MiB/s, done.\n",
            "Resolving deltas: 100% (236/236), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_d8V7oHAKy7",
        "outputId": "efed6e66-a4bc-453d-85c0-928987c76e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-24 12:15:53--  https://github.com/TIBHannover/GeoEstimation/releases/download/pytorch/epoch.014-val_loss.18.4833.ckpt\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/142275851/fc162380-3e05-11eb-9190-3ec4e4ff49c1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220924%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220924T121553Z&X-Amz-Expires=300&X-Amz-Signature=52a1015ac6178895715c836f02f1dff0c145a9c52e9a5903a32262e4db5da2c3&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=142275851&response-content-disposition=attachment%3B%20filename%3Depoch.014-val_loss.18.4833.ckpt&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-09-24 12:15:53--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/142275851/fc162380-3e05-11eb-9190-3ec4e4ff49c1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220924%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220924T121553Z&X-Amz-Expires=300&X-Amz-Signature=52a1015ac6178895715c836f02f1dff0c145a9c52e9a5903a32262e4db5da2c3&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=142275851&response-content-disposition=attachment%3B%20filename%3Depoch.014-val_loss.18.4833.ckpt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 571821130 (545M) [application/octet-stream]\n",
            "Saving to: â€˜models/base_M/epoch=014-val_loss=18.4833.ckptâ€™\n",
            "\n",
            "models/base_M/epoch 100%[===================>] 545.33M  7.40MB/s    in 59s     \n",
            "\n",
            "2022-09-24 12:16:52 (9.25 MB/s) - â€˜models/base_M/epoch=014-val_loss=18.4833.ckptâ€™ saved [571821130/571821130]\n",
            "\n",
            "--2022-09-24 12:16:52--  https://github.com/TIBHannover/GeoEstimation/releases/download/pytorch/hparams.yaml\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/142275851/d38e2980-3e05-11eb-9516-90659f3bd8d9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220924%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220924T121653Z&X-Amz-Expires=300&X-Amz-Signature=cee01f5268af3a80c1f976bce4b7441eb2066c1033e06b23f7760fbfeb1f0e50&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=142275851&response-content-disposition=attachment%3B%20filename%3Dhparams.yaml&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-09-24 12:16:53--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/142275851/d38e2980-3e05-11eb-9516-90659f3bd8d9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220924%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220924T121653Z&X-Amz-Expires=300&X-Amz-Signature=cee01f5268af3a80c1f976bce4b7441eb2066c1033e06b23f7760fbfeb1f0e50&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=142275851&response-content-disposition=attachment%3B%20filename%3Dhparams.yaml&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 828 [application/octet-stream]\n",
            "Saving to: â€˜models/base_M/hparams.yamlâ€™\n",
            "\n",
            "models/base_M/hpara 100%[===================>]     828  --.-KB/s    in 0s      \n",
            "\n",
            "2022-09-24 12:16:53 (46.5 MB/s) - â€˜models/base_M/hparams.yamlâ€™ saved [828/828]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download the model checkpoint & hyperparameters\n",
        "!mkdir -p models/base_M\n",
        "!wget https://github.com/TIBHannover/GeoEstimation/releases/download/pytorch/epoch.014-val_loss.18.4833.ckpt -O models/base_M/epoch=014-val_loss=18.4833.ckpt\n",
        "!wget https://github.com/TIBHannover/GeoEstimation/releases/download/pytorch/hparams.yaml -O models/base_M/hparams.yaml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "\n",
        "with open(\"/content/models/base_M/hparams.yaml\") as f:\n",
        "  list_doc = yaml.safe_load(f)\n",
        "\n",
        "list_doc[\"partitionings\"][\"files\"] = list(map(lambda x: \"/content/\" + x, list_doc[\"partitionings\"][\"files\"]))\n",
        "\n",
        "with open(\"/content/models/base_M/hparams.yaml\", \"w\") as f:\n",
        "  yaml.dump(list_doc, f)"
      ],
      "metadata": {
        "id": "bLl9QN7TEuQK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p resources/s2_cells\n",
        "!wget https://raw.githubusercontent.com/TIBHannover/GeoEstimation/original_tf/geo-cells/cells_50_5000.csv -O resources/s2_cells/cells_50_5000.csv\n",
        "!wget https://raw.githubusercontent.com/TIBHannover/GeoEstimation/original_tf/geo-cells/cells_50_2000.csv -O resources/s2_cells/cells_50_2000.csv\n",
        "!wget https://raw.githubusercontent.com/TIBHannover/GeoEstimation/original_tf/geo-cells/cells_50_1000.csv -O resources/s2_cells/cells_50_1000.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C66x3q6wCx1X",
        "outputId": "f5eb462e-ea59-4cf0-d26b-d483236fec82"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-24 12:17:05--  https://raw.githubusercontent.com/TIBHannover/GeoEstimation/original_tf/geo-cells/cells_50_5000.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 177214 (173K) [text/plain]\n",
            "Saving to: â€˜resources/s2_cells/cells_50_5000.csvâ€™\n",
            "\n",
            "resources/s2_cells/ 100%[===================>] 173.06K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-09-24 12:17:05 (10.4 MB/s) - â€˜resources/s2_cells/cells_50_5000.csvâ€™ saved [177214/177214]\n",
            "\n",
            "--2022-09-24 12:17:05--  https://raw.githubusercontent.com/TIBHannover/GeoEstimation/original_tf/geo-cells/cells_50_2000.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 389388 (380K) [text/plain]\n",
            "Saving to: â€˜resources/s2_cells/cells_50_2000.csvâ€™\n",
            "\n",
            "resources/s2_cells/ 100%[===================>] 380.26K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-09-24 12:17:06 (16.6 MB/s) - â€˜resources/s2_cells/cells_50_2000.csvâ€™ saved [389388/389388]\n",
            "\n",
            "--2022-09-24 12:17:06--  https://raw.githubusercontent.com/TIBHannover/GeoEstimation/original_tf/geo-cells/cells_50_1000.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 701771 (685K) [text/plain]\n",
            "Saving to: â€˜resources/s2_cells/cells_50_1000.csvâ€™\n",
            "\n",
            "resources/s2_cells/ 100%[===================>] 685.32K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-09-24 12:17:06 (19.5 MB/s) - â€˜resources/s2_cells/cells_50_1000.csvâ€™ saved [701771/701771]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.profiledir import LoggingConfigurable\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from typing import List\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "\n",
        "def dbscan_clustering(lats_and_longs: List[List[float]]) -> Tuple[np.ndarray, List[List[int]]]:\n",
        "  # DBSCAN clustering algorithm to cluster lats and longs\n",
        "  # Why DBSCAN? -- robust to outliers; needn't specify the number of clusters; density-based\n",
        "  lats_and_longs_standardized = StandardScaler().fit_transform(lats_and_longs)\n",
        "  db = DBSCAN(eps=0.5, min_samples=3)\n",
        "  db_fit = db.fit(lats_and_longs_standardized)\n",
        "  labels = db_fit.labels_\n",
        "  logging.info(f\"DBSCAN cluster labels: {labels}\")\n",
        "\n",
        "  return labels, lats_and_longs_standardized\n",
        "\n",
        "\n",
        "def plot_clusters(labels, lats_and_longs_standardized):\n",
        "  # not being called currently!\n",
        "  # plot DBSCAN clusters\n",
        "  plt.scatter(lats_and_longs_standardized[:, 0], lats_and_longs_standardized[:, 1], c=labels, cmap=\"Paired\")\n",
        "\n",
        "\n",
        "def data_engineering(image_dir: str) -> List[List[float]]:\n",
        "  inference_file_path = os.path.join(\"models/base_M\", f\"inference_{Path(os.path.join('/content', image_dir)).stem}.csv\")\n",
        "  inference_df = pd.read_csv(inference_file_path) \n",
        "  logging.info(f\"Inference DF: {inference_df.head()}\")\n",
        "\n",
        "  inference_df_lats_longs = inference_df[[\"pred_lat\", \"pred_lng\"]]\n",
        "  logging.info(f\"Inference Lats & Longs only DF: {inference_df_lats_longs}\")\n",
        "\n",
        "  lats_and_longs = inference_df_lats_longs.values.tolist()\n",
        "  return lats_and_longs\n",
        "\n",
        "\n",
        "def get_location(latitude: float, longitude: float) -> str:\n",
        "  geolocator = Nominatim(user_agent=\"geolocater\")\n",
        "  location = geolocator.geocode(f\"{latitude},{longitude}\")\n",
        "  return location\n",
        "\n",
        "\n",
        "def generate_prediction(image_dir: str) -> str:\n",
        "  %cd GeoEstimation\n",
        "\n",
        "  # generate predictions on all the video frames\n",
        "  subprocess.run([\"python\", \"-m\", \"classification.inference\", \"--image_dir\", os.path.join(\"/content\", image_dir), \"--checkpoint\", \"/content/models/base_M/epoch=014-val_loss=18.4833.ckpt\", \"--hparams\", \"/content/models/base_M/hparams.yaml\"], capture_output=True)\n",
        "\n",
        "  # go back to the /content directory\n",
        "  %cd ..\n",
        "\n",
        "  # data engineering\n",
        "  lats_and_longs = data_engineering(image_dir=image_dir)  \n",
        "\n",
        "  labels, lats_and_longs_standardized = dbscan_clustering(lats_and_longs=lats_and_longs)\n",
        "\n",
        "  # find the dense cluster\n",
        "  dense_cluster_label = max(set(labels), key=list(labels).count)\n",
        "  logging.info(f\"Dense cluster label: {dense_cluster_label}\")\n",
        "\n",
        "  # get data labels belonging to the dense cluster\n",
        "  indices = np.where(labels == dense_cluster_label)[0]\n",
        "  dense_cluster_data = list(map(lats_and_longs.__getitem__, indices))\n",
        "  logging.info(f\"Dense cluster data: {dense_cluster_data}\")\n",
        "\n",
        "  # fetch lat and long mean\n",
        "  lat_long_array = np.mean(np.array(dense_cluster_data, dtype=float), axis=0)\n",
        "  latitude, longitude = lat_long_array[0], lat_long_array[1]\n",
        "  logging.info(f\"Latitude: {latitude}, Longitutde: {longitude}\")\n",
        "\n",
        "  # get location\n",
        "  return get_location(latitude=latitude, longitude=longitude)"
      ],
      "metadata": {
        "id": "JONFjkWkA5ta"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def img_processor(img_file: str) -> str:\n",
        "  image_dir = os.path.join(image_parent_dir, os.path.basename(img_file).split(\".\")[0])\n",
        "\n",
        "  # clear the image directory before filling it up\n",
        "  shutil.rmtree(image_dir, ignore_errors=True)\n",
        "  os.makedirs(image_dir)\n",
        "  shutil.copy(img_file, image_dir)\n",
        "\n",
        "  return generate_prediction(image_dir=image_dir)\n",
        "\n",
        "\n",
        "def video_helper(video_file: str, info_dict: Dict[str, Any]) -> str:\n",
        "  # capture frames\n",
        "  frames_directory = capture_frames(video_file_path=video_file, info_dict=info_dict)\n",
        "  display_video_frames(frames_directory=frames_directory)\n",
        "\n",
        "  image_dir = frames_directory\n",
        "  return generate_prediction(image_dir=image_dir)\n",
        "\n",
        "\n",
        "def video_processor(video_file: str) -> str:\n",
        "  info_dict = {\"id\": os.path.basename(video_file).split(\".\")[0]}\n",
        "  return video_helper(video_file=video_file, info_dict=info_dict)\n",
        "\n",
        "\n",
        "def url_processor(url: str) -> str:\n",
        "  video_file, info_dict = extract_youtube_video(url=url)\n",
        "  return video_helper(video_file=video_file, info_dict=info_dict)"
      ],
      "metadata": {
        "id": "zBmqdRsYastQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# validation\n",
        "# url_processor(url=\"https://www.youtube.com/watch?v=ADt1LnbL2HI\")"
      ],
      "metadata": {
        "id": "76htoqT2T4GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradio\n",
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "q5QbHTu9AR3g"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "  gr.Markdown(\"# GeoLocator\")\n",
        "  gr.Markdown(\"An app that guesses the location of an image ðŸŒŒ, a video ðŸ“¹ or a YouTube link ðŸ”—.\")\n",
        "  with gr.Tab(\"Image\"):\n",
        "    with gr.Row():\n",
        "      img_input = gr.Image(type=\"filepath\")\n",
        "      img_text_output = gr.Textbox(label=\"Location\")\n",
        "    img_text_button = gr.Button(\"Go locate!\")\n",
        "  with gr.Tab(\"Video\"):\n",
        "    with gr.Row():\n",
        "      video_input = gr.Video(type=\"filepath\")\n",
        "      video_text_output = gr.Textbox(label=\"Location\")\n",
        "    video_text_button = gr.Button(\"Go locate!\")\n",
        "  with gr.Tab(\"YouTube Link\"):\n",
        "    with gr.Row():\n",
        "      url_input = gr.Textbox(label=\"YouTube video link\")\n",
        "      url_text_output = gr.Textbox(label=\"Location\")\n",
        "    url_text_button = gr.Button(\"Go locate!\")\n",
        "\n",
        "  img_text_button.click(img_processor, inputs=img_input, outputs=img_text_output)\n",
        "  video_text_button.click(video_processor, inputs=video_input, outputs=video_text_output)\n",
        "  url_text_button.click(url_processor, inputs=url_input, outputs=url_text_output)\n",
        "\n",
        "  examples = gr.Examples(examples=[\"https://www.youtube.com/watch?v=wxeQkJTZrsw\"], inputs=[url_input])\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 682
        },
        "id": "f5I_XZIjNL_c",
        "outputId": "4e9c1068-a20a-4a6c-9f8a-8770d1830370"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gradio/deprecation.py:40: UserWarning: The 'type' parameter has been deprecated. Use the Number component instead.\n",
            "  warnings.warn(value)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
            "Running on public URL: https://21030.gradio.app\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting, check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://21030.gradio.app\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<gradio.routes.App at 0x7efc66b52f50>,\n",
              " 'http://127.0.0.1:7863/',\n",
              " 'https://21030.gradio.app')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vm44v2Y8Na3R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}